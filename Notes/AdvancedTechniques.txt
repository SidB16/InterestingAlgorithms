Algorithmic Paradigms explored thus far, 
	1) Divide & Conquer (& Combine).
	2) Dynammic Programming.
	3) Greedy Algorithms.
*******************************DIVIDE-AND-CONQUER******************************************	
In general, divide-and-conquer algorithms: 
1) partition the problem into disjoint subproblems, 
2) solve the subproblems recursively, 
and 
3) then combine their solutions to solve the original problem.
*******************************DYNAMIC PROGRAMMING******************************************
Dynamic Programming (DP) applies when subproblems overlap, which means: subproblems share subproblems. 
Example: Recursive Fibonacci Algorithm.

Whereas a divide-and-conquer algorithm will do more work by re-computing an answer for a subproblem.
DP will compute the answer only once and store it in a table. Thus, it will avoid the cost the of re-computing the answer for a subproblem it has already solved.   

DP is most often used to solve Optimization Problems. 
These type of problems have many possible solutions.
Each solution has a value. 
Often we wish to find a solution that has an optimal (maximum or minimum) value.

A solution that has an optimal value, we call, an optimal solution.
Not "the" b/c there may be other solutions that also achieve an optimal value.

When designing a DP algorithm, we follow these four steps:
1) Characterize the structure of an optimal solution.
2) Recursively define the value of an optimal solution.
3) Compute the value of an optimal solution (usually done in a bottom-up fashion)
4) Construct an optimal solution from the computed information.

Top-Down View
Optimal Solutions to a problem incorporate Optimal Solutions to related subproblems, which we can solve independently.

OP  --> OS
↓			↓
SUBP --> OSUBS

Bottom-Up View
An optimal solution to a subproblem is a subset of an optimal solution to problem.